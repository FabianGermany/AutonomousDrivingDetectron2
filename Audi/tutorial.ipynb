{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the tutorial on the A2D2 Dataset. In this tutorial, we will learn how to work with the configuration file, the view item, the LiDAR data, camera images, and 3D bounding boxes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration JSON file plays an important role in processing the A2D2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open ('cams_lidars.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file contains three main items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sensor in 'lidars', 'cameras', and 'vehicle' has an associated 'view'. A view is a sensor coordinate system, defined by an origin, an x-axis, and a y-axis. These are specified in cartesian coordinates (in m) relative to an external coordinate system. Unless otherwise stated the external coordinate system is the car's frame of reference.\n",
    "\n",
    "'vehicle' contains a 'view' object specifying the frame of reference of the car. It also contains an 'ego-dimensions' object, which specifies the extension of the vehicle in the frame of reference of the car.\n",
    "\n",
    "The 'lidars' object contains objects specifying the extrinsic calibration parameters for each LiDAR sensor. Our car has five LiDAR sensors: 'front_left', 'front_center', 'front_right', 'rear_right', and 'rear_left'. Each LiDAR has a 'view' defining its pose in the frame of reference of the car.\n",
    "\n",
    "The 'cameras' object contains camera objects which specify their calibration parameters. The car has six cameras: 'front_left', 'front_center', 'front_right', 'side_right', 'rear_center' and 'side_left'. Each camera object contains: \n",
    "- 'view'- pose of the camera relative to the external coordinate system (frame of reference of the car)\n",
    "- 'Lens'- type of lens used. It can take two values: 'Fisheye' or 'Telecam'\n",
    "- 'CamMatrix' - the intrinsic camera matrix of undistorted camera images\n",
    "- 'CamMatrixOriginal' - the intrinsic camera matrix of original (distorted) camera images\n",
    "- 'Distortion' - distortion parameters of original (distorted) camera images\n",
    "- 'Resolution' - resolution (columns, rows) of camera images (same for original and undistorted images) \n",
    "- 'tstamp_delay'- specifies a known delay in microseconds between actual camera frame times (default: 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the contents of 'vehicle':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['vehicle'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise for LiDAR sensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['lidars'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the names of the LiDAR sensors mounted on the car. For example, the configuration parameters for the front_left LiDAR sensor can be accessed using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['lidars']['front_left']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The camera sensors mounted on the car can be obtained using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['cameras'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration parameters for a particular camera can be accessed using e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['cameras']['front_left']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with view objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the vehicle and each sensor in the configuration file have a 'view' object. A view specifies the pose of a sensor relative to an external coordinate system, here the frame of reference of the car. In the following we use the term 'global' interchangeably with 'frame of reference of the car'.\n",
    "\n",
    "A view associated with a sensor can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = config['cameras']['front_left']['view']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a small constant to avoid errors due to small vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1.0e-10 # norm should not be small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions get the axes and origin of a view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_axes_of_a_view(view):\n",
    "    x_axis = view['x-axis']\n",
    "    y_axis = view['y-axis']\n",
    "     \n",
    "    x_axis_norm = la.norm(x_axis)\n",
    "    y_axis_norm = la.norm(y_axis)\n",
    "    \n",
    "    if (x_axis_norm < EPSILON or y_axis_norm < EPSILON):\n",
    "        raise ValueError(\"Norm of input vector(s) too small.\")\n",
    "        \n",
    "    # normalize the axes\n",
    "    x_axis = x_axis / x_axis_norm\n",
    "    y_axis = y_axis / y_axis_norm\n",
    "    \n",
    "    # make a new y-axis which lies in the original x-y plane, but is orthogonal to x-axis\n",
    "    y_axis = y_axis - x_axis * np.dot(y_axis, x_axis)\n",
    " \n",
    "    # create orthogonal z-axis\n",
    "    z_axis = np.cross(x_axis, y_axis)\n",
    "    \n",
    "    # calculate and check y-axis and z-axis norms\n",
    "    y_axis_norm = la.norm(y_axis)\n",
    "    z_axis_norm = la.norm(z_axis)\n",
    "    \n",
    "    if (y_axis_norm < EPSILON) or (z_axis_norm < EPSILON):\n",
    "        raise ValueError(\"Norm of view axis vector(s) too small.\")\n",
    "        \n",
    "    # make x/y/z-axes orthonormal\n",
    "    y_axis = y_axis / y_axis_norm\n",
    "    z_axis = z_axis / z_axis_norm\n",
    "    \n",
    "    return x_axis, y_axis, z_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_origin_of_a_view(view):\n",
    "    return view['origin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A homogeneous transformation matrix from view point to global coordinates (inverse \"extrinsic\" matrix) can be obtained as follows. Note that this matrix contains the axes and the origin in its columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_to_global(view):\n",
    "    # get axes\n",
    "    x_axis, y_axis, z_axis = get_axes_of_a_view(view)\n",
    "    \n",
    "    # get origin \n",
    "    origin = get_origin_of_a_view(view)\n",
    "    transform_to_global = np.eye(4)\n",
    "    \n",
    "    # rotation\n",
    "    transform_to_global[0:3, 0] = x_axis\n",
    "    transform_to_global[0:3, 1] = y_axis\n",
    "    transform_to_global[0:3, 2] = z_axis\n",
    "    \n",
    "    # origin\n",
    "    transform_to_global[0:3, 3] = origin\n",
    "    \n",
    "    return transform_to_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the view defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_to_global = get_transform_to_global(view)\n",
    "print (transform_to_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homogeneous transformation matrix from global coordinates to view point coordinates (\"extrinsic\" matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_from_global(view):\n",
    "   # get transform to global\n",
    "   transform_to_global = get_transform_to_global(view)\n",
    "   trans = np.eye(4)\n",
    "   rot = np.transpose(transform_to_global[0:3, 0:3])\n",
    "   trans[0:3, 0:3] = rot\n",
    "   trans[0:3, 3] = np.dot(rot, -transform_to_global[0:3, 3])\n",
    "    \n",
    "   return trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the view defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_from_global = get_transform_from_global(view)\n",
    "print(transform_from_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform_to_global and transform_from_global matrices should be the inverse of one another.\n",
    "Check that muliplying them results in an identity matrix (subject to numerical precision):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.matmul(transform_from_global, transform_to_global))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global-to-view rotation matrix can be obtained using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rot_from_global(view):\n",
    "    # get transform to global\n",
    "    transform_to_global = get_transform_to_global(view)\n",
    "    # get rotation\n",
    "    rot =  np.transpose(transform_to_global[0:3, 0:3])\n",
    "    \n",
    "    return rot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the view defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_from_global = get_rot_from_global(view)\n",
    "print(rot_from_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rotation matrix from this view point to the global coordinate system can be obtained using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rot_to_global(view):\n",
    "    # get transform to global\n",
    "    transform_to_global = get_transform_to_global(view)\n",
    "    # get rotation\n",
    "    rot = transform_to_global[0:3, 0:3]\n",
    "    \n",
    "    return rot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the view defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_to_global = get_rot_to_global(view)\n",
    "print(rot_to_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how we can calculate a rotation matrix from a source view to a target view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rot_from_to(src, target):\n",
    "    rot = np.dot(get_rot_from_global(target), get_rot_to_global(src))\n",
    "    \n",
    "    return rot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rotation matrix from front left camera to front right camera "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_view = config['cameras']['front_left']['view']\n",
    "target_view = config['cameras']['front_right']['view']\n",
    "rot = rot_from_to(src_view, target_view)\n",
    "print(rot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rotation matrix in the opposite direction (front right camera -> front left camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot = rot_from_to(target_view, src_view)\n",
    "print(rot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same manner, we can also calculate a transformation matrix from a source view to a target view. This will give us a 4x4 homogeneous transformation matrix describing the total transformation (rotation and shift) from the source view coordinate system into the target view coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_from_to(src, target):\n",
    "    transform = np.dot(get_transform_from_global(target), \\\n",
    "                       get_transform_to_global(src))\n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformation matrix from front left camera to front right camera  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_view = config['cameras']['front_left']['view']\n",
    "target_view = config['cameras']['front_right']['view']\n",
    "trans = transform_from_to(src_view, target_view)\n",
    "print(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformation matrix in the opposite direction (front right camera -> front left camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transt = transform_from_to(target_view, src_view)\n",
    "print (transt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the product of the two opposite transformations results in a near identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.matmul(trans, transt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that by using views we can transform coordinates from one sensor to another, or from a sensor to global global coordinates (and vice versa). In the following section, we read point clouds corresponding to all cameras. The point clouds are in camera view coordinates. In order to get a coherent view of the point clouds, we need to transform them into global coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with LiDAR data\n",
    "First, read a LiDAR point cloud corresponding to the front center camera. The LiDAR data is saved in compressed numpy format, which can be read as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import glob\n",
    "\n",
    "root_path = './camera_lidar_semantic_bboxes/'\n",
    "# get the list of files in lidar directory\n",
    "file_names = sorted(glob.glob(join(root_path, '*/lidar/cam_front_center/*.npz')))\n",
    "\n",
    "# select the lidar point cloud\n",
    "file_name_lidar = file_names[7]\n",
    "\n",
    "# read the lidar data\n",
    "lidar_front_center = np.load(file_name_lidar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the LiDAR data using the LiDAR points within the field of view of the front center camera. \n",
    "List keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(lidar_front_center.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get 3D point measurements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = lidar_front_center['points']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get reflectance measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflectance = lidar_front_center['reflectance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = lidar_front_center['timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get coordinates of LiDAR points in image space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = lidar_front_center['row']\n",
    "cols = lidar_front_center['col']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get distance and depth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = lidar_front_center['distance']\n",
    "depth = lidar_front_center['depth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the car is equipped with five LiDAR sensors, you can get the LiDAR sensor ID of each point using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_ids = lidar_front_center['lidar_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of visualizing point clouds is to use the Open3D library. The library supports beyond visualization other functionalities useful for point cloud processing. For more information on the library please refer to http://www.open3d.org/docs/release/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the LiDAR point clouds, we need to create an Open3D point cloud from the 3D points and reflectance values. The following function generates colors based on the reflectance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of RGB colour values from the given array of reflectance values\n",
    "def colours_from_reflectances(reflectances):\n",
    "    return np.stack([reflectances, reflectances, reflectances], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create Open3D point clouds for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_open3d_pc(lidar, cam_image=None):\n",
    "    # create open3d point cloud\n",
    "    pcd = o3.geometry.PointCloud()\n",
    "    \n",
    "    # assign point coordinates\n",
    "    pcd.points = o3.utility.Vector3dVector(lidar['points'])\n",
    "    \n",
    "    # assign colours\n",
    "    if cam_image is None:\n",
    "        median_reflectance = np.median(lidar['reflectance'])\n",
    "        colours = colours_from_reflectances(lidar['reflectance']) / (median_reflectance * 5)\n",
    "        \n",
    "        # clip colours for visualisation on a white background\n",
    "        colours = np.clip(colours, 0, 0.75)\n",
    "    else:\n",
    "        rows = (lidar['row'] + 0.5).astype(np.int)\n",
    "        cols = (lidar['col'] + 0.5).astype(np.int)\n",
    "        colours = cam_image[rows, cols, :] / 255.0\n",
    "        \n",
    "    pcd.colors = o3.utility.Vector3dVector(colours)\n",
    "    \n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Open3D point cloud for the LiDAR data associated with the front center camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_front_center = create_open3d_pc(lidar_front_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.visualization.draw_geometries([pcd_front_center])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us transform LiDAR points from the camera view to the global view.\n",
    "\n",
    "First, read the view for the front center camera from the configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_view_front_center = config['cameras']['front_center']['view']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vehicle view is the global view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_view = target_view = config['vehicle']['view']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function maps LiDAR data from one view to another. Note the use of the function 'transform_from_to'. LiDAR data is provided in a camera reference frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_lidar_from_to(lidar, src_view, target_view):\n",
    "    lidar = dict(lidar)\n",
    "    trans = transform_from_to(src_view, target_view)\n",
    "    points = lidar['points']\n",
    "    points_hom = np.ones((points.shape[0], 4))\n",
    "    points_hom[:, 0:3] = points\n",
    "    points_trans = (np.dot(trans, points_hom.T)).T \n",
    "    lidar['points'] = points_trans[:,0:3]\n",
    "    \n",
    "    return lidar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now project the LiDAR points to the global frame (the vehicle frame of reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_front_center = project_lidar_from_to(lidar_front_center,\\\n",
    "                                          src_view_front_center, \\\n",
    "                                          vehicle_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create open3d point cloud for visualizing the transformed points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_front_center = create_open3d_pc(lidar_front_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.visualization.draw_geometries([pcd_front_center])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more visible transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_view = config['lidars']['rear_right']['view']\n",
    "lidar_front_center = project_lidar_from_to(lidar_front_center,\\\n",
    "                                          src_view_front_center, \\\n",
    "                                          target_view)\n",
    "pcd_front_center = create_open3d_pc(lidar_front_center)\n",
    "o3.visualization.draw_geometries([pcd_front_center])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages for reading, saving and showing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the image corresponding to the above point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_file_name_from_lidar_file_name(file_name_lidar):\n",
    "    file_name_image = file_name_lidar.split('/')\n",
    "    file_name_image = file_name_image[-1].split('.')[0]\n",
    "    file_name_image = file_name_image.split('_')\n",
    "    file_name_image = file_name_image[0] + '_' + \\\n",
    "                        'camera_' + \\\n",
    "                        file_name_image[2] + '_' + \\\n",
    "                        file_name_image[3] + '.png'\n",
    "\n",
    "    return file_name_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_name = file_name_lidar.split('/')[2]\n",
    "file_name_image = extract_image_file_name_from_lidar_file_name(file_name_lidar)\n",
    "file_name_image = join(root_path, seq_name, 'camera/cam_front_center/', file_name_image)\n",
    "image_front_center = cv2.imread(file_name_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_front_center = cv2.cvtColor(image_front_center, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt.fig = pt.figure(figsize=(15, 15))\n",
    "\n",
    "# display image from front center camera\n",
    "pt.imshow(image_front_center)\n",
    "pt.axis('off')\n",
    "pt.title('front center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to map point clouds onto images, or in order to color point clouds using colors drived from images, we need to perform distortion correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort_image(image, cam_name):\n",
    "    if cam_name in ['front_left', 'front_center', \\\n",
    "                    'front_right', 'side_left', \\\n",
    "                    'side_right', 'rear_center']:\n",
    "        # get parameters from config file\n",
    "        intr_mat_undist = \\\n",
    "                  np.asarray(config['cameras'][cam_name]['CamMatrix'])\n",
    "        intr_mat_dist = \\\n",
    "                  np.asarray(config['cameras'][cam_name]['CamMatrixOriginal'])\n",
    "        dist_parms = \\\n",
    "                  np.asarray(config['cameras'][cam_name]['Distortion'])\n",
    "        lens = config['cameras'][cam_name]['Lens']\n",
    "        \n",
    "        if (lens == 'Fisheye'):\n",
    "            return cv2.fisheye.undistortImage(image, intr_mat_dist,\\\n",
    "                                      D=dist_parms, Knew=intr_mat_undist)\n",
    "        elif (lens == 'Telecam'):\n",
    "            return cv2.undistort(image, intr_mat_dist, \\\n",
    "                      distCoeffs=dist_parms, newCameraMatrix=intr_mat_undist)\n",
    "        else:\n",
    "            return image\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undist_image_front_center = undistort_image(image_front_center, 'front_center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.fig = pt.figure(figsize=(15, 15))\n",
    "pt.imshow(undist_image_front_center)\n",
    "pt.axis('off')\n",
    "pt.title('front center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image has a timestamp and a LiDAR point cloud associated with it. The timestamp information is saved for each image in JSON format. Let us open the file for the front center camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_image_info = file_name_image.replace(\".png\", \".json\")\n",
    "\n",
    "def read_image_info(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        image_info = json.load(f)\n",
    "        \n",
    "    return image_info\n",
    "\n",
    "image_info_front_center = read_image_info(file_name_image_info)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the information for the front center camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(image_info_front_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the camera info contains the camera name, the time stamp in TAI (international atomic time) and a dictionary associating the LiDAR IDs with names of the LiDARs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LiDAR points are already mapped onto the undistorted images. The rows and columns of the corresponding pixels are saved in the lidar data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us list the keys once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_front_center.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the timestamp of each point in the LiDAR measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(lidar_front_center['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we also see the timestamps of each measurement point in TAI. The camera is lagging behind the LiDAR points, i.e. the LiDAR measurements are taken before the corresponding image is captured. (timestamp_lidar-timestamp_camera)/(1000000) gives us the time difference between the measurement times of lidar data and the corresponding camera frame in seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lidar_id_vs_delat_t(image_info, lidar):\n",
    "    timestamps_lidar = lidar['timestamp']\n",
    "    timestamp_camera = image_info['cam_tstamp']\n",
    "    time_diff_in_sec = (timestamps_lidar - timestamp_camera) / (1e6)\n",
    "    lidar_ids = lidar ['lidar_id']\n",
    "    pt.fig = pt.figure(figsize=(15, 5))\n",
    "    pt.plot(time_diff_in_sec, lidar_ids, 'go', ms=2)\n",
    "    pt.grid(True)\n",
    "    ticks = np.arange(len(image_info['lidar_ids'].keys()))\n",
    "    ticks_name = []\n",
    "    for key in ticks:\n",
    "        ticks_name.append(image_info['lidar_ids'][str(key)])\n",
    "    pt.yticks(ticks, tuple(ticks_name))\n",
    "    pt.ylabel('LiDAR sensor')\n",
    "    pt.xlabel('delta t in sec')\n",
    "    pt.title(image_info['cam_name'])\n",
    "    pt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the lidar_ids versus the time difference  for the front center camera we obtain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lidar_id_vs_delat_t(image_info_front_center, lidar_front_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use col and row to map the LiDAR data onto images. The first function we use converts HSV to RGB. Please refere to the wikipedia article https://en.wikipedia.org/wiki/HSL_and_HSV for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsv_to_rgb(h, s, v):\n",
    "    if s == 0.0:\n",
    "        return v, v, v\n",
    "    \n",
    "    i = int(h * 6.0)\n",
    "    f = (h * 6.0) - i\n",
    "    p = v * (1.0 - s)\n",
    "    q = v * (1.0 - s * f)\n",
    "    t = v * (1.0 - s * (1.0 - f))\n",
    "    i = i % 6\n",
    "    \n",
    "    if i == 0:\n",
    "        return v, t, p\n",
    "    if i == 1:\n",
    "        return q, v, p\n",
    "    if i == 2:\n",
    "        return p, v, t\n",
    "    if i == 3:\n",
    "        return p, q, v\n",
    "    if i == 4:\n",
    "        return t, p, v\n",
    "    if i == 5:\n",
    "        return v, p, q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function visualizes the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_lidar_points_onto_image(image_orig, lidar, pixel_size=3, pixel_opacity=1):\n",
    "    image = np.copy(image_orig)\n",
    "    \n",
    "    # get rows and cols\n",
    "    rows = (lidar['row'] + 0.5).astype(np.int)\n",
    "    cols = (lidar['col'] + 0.5).astype(np.int)\n",
    "  \n",
    "    # lowest distance values to be accounted for in colour code\n",
    "    MIN_DISTANCE = np.min(lidar['distance'])\n",
    "    # largest distance values to be accounted for in colour code\n",
    "    MAX_DISTANCE = np.max(lidar['distance'])\n",
    "\n",
    "    # get distances\n",
    "    distances = lidar['distance']  \n",
    "    # determine point colours from distance\n",
    "    colours = (distances - MIN_DISTANCE) / (MAX_DISTANCE - MIN_DISTANCE)\n",
    "    colours = np.asarray([np.asarray(hsv_to_rgb(0.75 * c, \\\n",
    "                        np.sqrt(pixel_opacity), 1.0)) for c in colours])\n",
    "    pixel_rowoffs = np.indices([pixel_size, pixel_size])[0] - pixel_size // 2\n",
    "    pixel_coloffs = np.indices([pixel_size, pixel_size])[1] - pixel_size // 2\n",
    "    canvas_rows = image.shape[0]\n",
    "    canvas_cols = image.shape[1]\n",
    "    for i in range(len(rows)):\n",
    "        pixel_rows = np.clip(rows[i] + pixel_rowoffs, 0, canvas_rows - 1)\n",
    "        pixel_cols = np.clip(cols[i] + pixel_coloffs, 0, canvas_cols - 1)\n",
    "        image[pixel_rows, pixel_cols, :] = \\\n",
    "                (1. - pixel_opacity) * \\\n",
    "                np.multiply(image[pixel_rows, pixel_cols, :], \\\n",
    "                colours[i]) + pixel_opacity * 255 * colours[i]\n",
    "    return image.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the mapping of the LiDAR point clouds onto the front center image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = map_lidar_points_onto_image(undist_image_front_center, lidar_front_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.fig = pt.figure(figsize=(20, 20))\n",
    "pt.imshow(image)\n",
    "pt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result can be obtained by mapping the LiDAR point clouds using intrinsic camera parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us finally open the semantic segmentation label corresponding to the above image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_semantic_file_name_from_image_file_name(file_name_image):\n",
    "    file_name_semantic_label = file_name_image.split('/')\n",
    "    file_name_semantic_label = file_name_semantic_label[-1].split('.')[0]\n",
    "    file_name_semantic_label = file_name_semantic_label.split('_')\n",
    "    file_name_semantic_label = file_name_semantic_label[0] + '_' + \\\n",
    "                  'label_' + \\\n",
    "                  file_name_semantic_label[2] + '_' + \\\n",
    "                  file_name_semantic_label[3] + '.png'\n",
    "    \n",
    "    return file_name_semantic_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_name = file_name_lidar.split('/')[2]\n",
    "file_name_semantic_label = extract_semantic_file_name_from_image_file_name(file_name_image)\n",
    "file_name_semantic_label = join(root_path, seq_name, 'label/cam_front_center/', file_name_semantic_label)\n",
    "semantic_image_front_center = cv2.imread(file_name_semantic_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the semantic segmentation label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_image_front_center = cv2.cvtColor(semantic_image_front_center, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.fig = pt.figure(figsize=(15, 15))\n",
    "pt.imshow(semantic_image_front_center)\n",
    "pt.axis('off')\n",
    "pt.title('label front center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the semantic segmentation label to colour lidar points. This creates a 3D semantic label for a given frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to undistort the semantic segmentation label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "semantic_image_front_center_undistorted = undistort_image(semantic_image_front_center, 'front_center')\n",
    "pt.fig = pt.figure(figsize=(15, 15))\n",
    "pt.imshow(semantic_image_front_center_undistorted)\n",
    "pt.axis('off')\n",
    "pt.title('label front center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_lidar_colored = create_open3d_pc(lidar_front_center, semantic_image_front_center_undistorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the coloured lidar points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3.visualization.draw_geometries([pcd_lidar_colored])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with 3D bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start working with 3D bounding boxes, we need some utility functions. The first utility function we need is the conversion from axis-angle representation into rotation matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skew_sym_matrix(u):\n",
    "    return np.array([[    0, -u[2],  u[1]], \n",
    "                     [ u[2],     0, -u[0]], \n",
    "                     [-u[1],  u[0],    0]])\n",
    "\n",
    "def axis_angle_to_rotation_mat(axis, angle):\n",
    "    return np.cos(angle) * np.eye(3) + \\\n",
    "        np.sin(angle) * skew_sym_matrix(axis) + \\\n",
    "        (1 - np.cos(angle)) * np.outer(axis, axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the bounding boxes corresponding to the frame. We can read the bounding boxes as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_bounding_boxes(file_name_bboxes):\n",
    "    # open the file\n",
    "    with open (file_name_bboxes, 'r') as f:\n",
    "        bboxes = json.load(f)\n",
    "        \n",
    "    boxes = [] # a list for containing bounding boxes  \n",
    "    print(bboxes.keys())\n",
    "    \n",
    "    for bbox in bboxes.keys():\n",
    "        bbox_read = {} # a dictionary for a given bounding box\n",
    "        bbox_read['class'] = bboxes[bbox]['class']\n",
    "        bbox_read['truncation']= bboxes[bbox]['truncation']\n",
    "        bbox_read['occlusion']= bboxes[bbox]['occlusion']\n",
    "        bbox_read['alpha']= bboxes[bbox]['alpha']\n",
    "        bbox_read['top'] = bboxes[bbox]['2d_bbox'][0]\n",
    "        bbox_read['left'] = bboxes[bbox]['2d_bbox'][1]\n",
    "        bbox_read['bottom'] = bboxes[bbox]['2d_bbox'][2]\n",
    "        bbox_read['right']= bboxes[bbox]['2d_bbox'][3]\n",
    "        bbox_read['center'] =  np.array(bboxes[bbox]['center'])\n",
    "        bbox_read['size'] =  np.array(bboxes[bbox]['size'])\n",
    "        angle = bboxes[bbox]['rot_angle']\n",
    "        axis = np.array(bboxes[bbox]['axis'])\n",
    "        bbox_read['rotation'] = axis_angle_to_rotation_mat(axis, angle) \n",
    "        boxes.append(bbox_read)\n",
    "\n",
    "    return boxes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us read bounding boxes corresponding to the above image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bboxes_file_name_from_image_file_name(file_name_image):\n",
    "    file_name_bboxes = file_name_image.split('/')\n",
    "    file_name_bboxes = file_name_bboxes[-1].split('.')[0]\n",
    "    file_name_bboxes = file_name_bboxes.split('_')\n",
    "    file_name_bboxes = file_name_bboxes[0] + '_' + \\\n",
    "                  'label3D_' + \\\n",
    "                  file_name_bboxes[2] + '_' + \\\n",
    "                  file_name_bboxes[3] + '.json'\n",
    "    \n",
    "    return file_name_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_name = file_name_lidar.split('/')[2]\n",
    "file_name_bboxes = extract_bboxes_file_name_from_image_file_name(file_name_image)\n",
    "file_name_bboxes = join(root_path, seq_name, 'label3D/cam_front_center/', file_name_bboxes)\n",
    "print (file_name_bboxes)\n",
    "boxes = read_bounding_boxes(file_name_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate the vertices of a bounding box in a particular order. The following function generates the vertices in the following order:\n",
    "[bottom_rear_left, bottom_front_left, bottom_front_right, bottom_rear_right, top_rear_left, top_front_left, top_front_right, top_rear_right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(bbox):\n",
    "    half_size = bbox['size'] / 2.\n",
    "    \n",
    "    if half_size[0] > 0:\n",
    "        # calculate unrotated corner point offsets relative to center\n",
    "        brl = np.asarray([-half_size[0], +half_size[1], -half_size[2]])\n",
    "        bfl = np.asarray([+half_size[0], +half_size[1], -half_size[2]])\n",
    "        bfr = np.asarray([+half_size[0], -half_size[1], -half_size[2]])\n",
    "        brr = np.asarray([-half_size[0], -half_size[1], -half_size[2]])\n",
    "        trl = np.asarray([-half_size[0], +half_size[1], +half_size[2]])\n",
    "        tfl = np.asarray([+half_size[0], +half_size[1], +half_size[2]])\n",
    "        tfr = np.asarray([+half_size[0], -half_size[1], +half_size[2]])\n",
    "        trr = np.asarray([-half_size[0], -half_size[1], +half_size[2]])\n",
    "     \n",
    "        # rotate points\n",
    "        points = np.asarray([brl, bfl, bfr, brr, trl, tfl, tfr, trr])\n",
    "        points = np.dot(points, bbox['rotation'].T)\n",
    "        \n",
    "        # add center position\n",
    "        points = points + bbox['center']\n",
    "  \n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get the vertices of the first bounding box in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = get_points(boxes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the bounding boxes in the LiDAR space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or update open3d wire frame geometry for the given bounding boxes\n",
    "def _get_bboxes_wire_frames(bboxes, linesets=None, color=None):\n",
    "\n",
    "    num_boxes = len(bboxes)\n",
    "        \n",
    "    # initialize linesets, if not given\n",
    "    if linesets is None:\n",
    "        linesets = [o3.geometry.LineSet() for _ in range(num_boxes)]\n",
    "\n",
    "    # set default color\n",
    "    if color is None:\n",
    "        #color = [1, 0, 0]\n",
    "        color = [0, 0, 1]\n",
    "\n",
    "    assert len(linesets) == num_boxes, \"Number of linesets must equal number of bounding boxes\"\n",
    "\n",
    "    # point indices defining bounding box edges\n",
    "    lines = [[0, 1], [1, 2], [2, 3], [3, 0],\n",
    "             [0, 4], [1, 5], [2, 6], [3, 7],\n",
    "             [4, 5], [5, 6], [6, 7], [7, 4], \n",
    "             [5, 2], [1, 6]]\n",
    "\n",
    "    # loop over all bounding boxes\n",
    "    for i in range(num_boxes):\n",
    "        # get bounding box corner points\n",
    "        points = get_points(bboxes[i])\n",
    "        # update corresponding Open3d line set\n",
    "        colors = [color for _ in range(len(lines))]\n",
    "        line_set = linesets[i]\n",
    "        line_set.points = o3.utility.Vector3dVector(points)\n",
    "        line_set.lines = o3.utility.Vector2iVector(lines)\n",
    "        line_set.colors = o3.utility.Vector3dVector(colors)\n",
    "\n",
    "    return linesets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load LiDAR data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_front_center = np.load(file_name_lidar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Open3D point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_front_center = create_open3d_pc(lidar_front_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw LiDAR points with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_to_draw = []\n",
    "entities_to_draw.append(pcd_front_center)\n",
    "\n",
    "for bbox in boxes:\n",
    "    linesets = _get_bboxes_wire_frames([bbox], color=(255,0,0))\n",
    "    entities_to_draw.append(linesets[0])\n",
    "    \n",
    "o3.visualization.draw_geometries(entities_to_draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize both the bounding boxes and 3D semantic segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_lidar_colored = create_open3d_pc(lidar_front_center, semantic_image_front_center_undistorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_to_draw = []\n",
    "entities_to_draw.append(pcd_lidar_colored)\n",
    "\n",
    "for bbox in boxes:\n",
    "    linesets = _get_bboxes_wire_frames([bbox], color=(255, 0, 0))\n",
    "    entities_to_draw.append(linesets[0])\n",
    "    \n",
    "o3.visualization.draw_geometries(entities_to_draw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
