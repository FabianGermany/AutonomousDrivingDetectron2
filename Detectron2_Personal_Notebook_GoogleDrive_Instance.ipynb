{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detectron2_Personal_Notebook_GoogleDrive_Instance",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python376jvsc74a57bd05926bfc64e41ec31041676ddb3494774781d5e749dbe29e832f164ef241aaca0",
      "display_name": "Python 3.7.6 64-bit"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabianGermany/AutonomousDrivingDetectron2/blob/main/Detectron2_Personal_Notebook_GoogleDrive_Instance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR"
      },
      "source": [
        "# Basic information: Detectron2 algorithm used for Audi A2D2 dataset\n",
        "\n",
        "<img src=\"https://github.com/FabianGermany/AutonomousDrivingDetectron2/blob/main/output_data/example_output_object_detection_pretrained.jpg?raw=1\" width=\"900\">\n",
        "\n",
        "In the Colab/Jupyter notebook, we will\n",
        "* choose a pretrained model available in Detectron2 framework\n",
        "* shortly test the default pre-trained Detectron2 model on a single picture for test and demonstration purposes\n",
        "* shortly test the access to the Audi A2D2 dataset and parse to dataset to a format we need later\n",
        "* train the model on the Audi A2D2 dataset with training images and\n",
        "* test the model trained on Audi A2D2 with test images in order to evaluate this model\n",
        "* run the default and the trained model an an exemplary video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltj3joZvSAEP"
      },
      "source": [
        "# 1 General preparations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilq1pJ5P4t-s"
      },
      "source": [
        "General stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2n56T_u4wUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba6e71f-b341-4b67-c135-7343715901bd"
      },
      "source": [
        "#beautiful print for Colab steps\n",
        "def statement_done():\n",
        "  print(\"\\n\")\n",
        "  print(30 * \"*\")\n",
        "  print(\"This step is done.\")\n",
        "  print(30 * \"*\")\n",
        "  \n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "******************************\n",
            "This step is done.\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQRdgFa0hyy-"
      },
      "source": [
        "Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5VR-41Vh0Se",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afdd40ba-eb0f-4381-d6a0-d026ef2b3828"
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') #or drive.mount('/content/drive')\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "\n",
            "\n",
            "******************************\n",
            "This step is done.\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CYoqkr6Gwa_"
      },
      "source": [
        "Some of the packages like OpenCV are already installed on Colab Server on default and only need to be imported. Other need to be installed via pip. Since notebooks sometimes need to reconnect and re-install takes a lot of time, you can keep the installation on Google Drive using this solution, but this doesn't work well with Detectron and torch 1.8...\n",
        "\n",
        "(Warning: This only works if the notebook is opened in content/notebooks \n",
        "folder, not if it's opened from GitHub!)\n",
        "\n",
        "So all in all, I recommend to set the boolean `local_install` to false."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94tmN9qeVH3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d83f01-df8d-40b0-b668-044970f7fb73"
      },
      "source": [
        "#local_install = True\n",
        "local_install = False\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "******************************\n",
            "This step is done.\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpE3t1v2e_tF"
      },
      "source": [
        "Also make sure to choose the desired mode. It's recommended to set `dataset_json_available`and `load_existing_trained_model` to `True` if the script has already been run successfully at least once and we just want to perform an evaluation. Putting this to `True` this implies that we simply load our previosuly stored dataset .json and our trained model so we don't need to recalculate everything for running the evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_Lu67WSVVXI"
      },
      "source": [
        "#decide whether to re-generate the A2D2 dict data or just load it if it was already calculated before and stored in cloud\n",
        "dataset_json_available = True\n",
        "#dataset_json_available = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW5NlFMLR7XQ"
      },
      "source": [
        "#decide whether to used existing trained model (stored in output folder) or re-learn the model\n",
        "load_existing_trained_model = True\n",
        "#load_existing_trained_model = False\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfAKPmDLV41u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb6c1bd-77b2-446d-d9c1-071bdd3e23a8"
      },
      "source": [
        "#only do this if local installation\n",
        "if(local_install):\n",
        "\n",
        "  #create a path to save the modules in Google Drive\n",
        "  nb_path = '/content/notebooks'\n",
        "  os.symlink('/content/gdrive/My Drive/Colab Notebooks', nb_path)\n",
        "  sys.path.insert(0,nb_path)\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "******************************\n",
            "This step is done.\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ4bwPn_WGde"
      },
      "source": [
        "Install packages/dependencies and import them\n",
        "Some of them or the desired specific versions may not be installed by default on Colab, so we need the *!pip install* commands. For example we need a specific version of pytorch (pytorch 1.8). Pytorch version 1.9 is already\n",
        "installed on Colab, but is not compatible with Detectron2. See more [here](https://detectron2.readthedocs.io/en/latest/tutorials/install.html).\n",
        "\n",
        "The *--target=$nb_path* means that it will be installed locally to the Google Drive.\n",
        "\n",
        "If *--target=$nb_path* is activated, you only need to run this once.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXh34gULGyU_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75472514-d6a3-43fb-d3c6-e7719b5ab7cc"
      },
      "source": [
        "#only do this if local installation\n",
        "if (local_install):\n",
        "  !pip install --target=$nb_path pyyaml==5.1\n",
        "  !pip install --target=$nb_path torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "  !pip install --target=$nb_path detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "  \n",
        "#server installation\n",
        "else: #not local_install\n",
        "  !pip install pyyaml==5.1 #if locally: !pip install --target=$nb_path pyyaml==5.1\n",
        "  !pip install torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "  !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "\n",
        "#exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime  \n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyyaml==5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 13.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 18.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 10.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 143kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 163kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 174kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 184kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 194kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 204kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 225kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 235kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 245kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 256kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 5.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44091 sha256=f05becaa22226cd0b1805787cad4e0d98e362238a859cad5d9a206f09c8f7f5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.1\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5MB)\n",
            "\u001b[K     |████████████████████████████████| 763.5MB 23kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.0+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.9.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 136kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0+cu101) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "Successfully installed torch-1.8.0+cu101 torchvision-0.9.0+cu101\n",
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
            "Collecting detectron2\n",
            "\u001b[?25l  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/detectron2-0.4%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.2MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2MB 795kB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2) (4.41.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (7.1.2)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2) (3.2.2)\n",
            "Collecting fvcore<0.1.4,>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/68/2bacb80e13c4084dfc37fec8f17706a1de4c248157561ff33e463399c4f5/fvcore-0.1.3.post20210317.tar.gz (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.16.0)\n",
            "Collecting omegaconf>=2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/96/1966b48bfe6ca64bfadfa7bcc9a8d73c5d83b4be769321fcc5d617abeb0c/omegaconf-2.1.0-py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.7MB/s \n",
            "\u001b[?25hCollecting iopath>=0.1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/af/20/65dd9bd25a1eb7fa35b5ae38d289126af065f8a0c1f6a90564f4bff0f89d/iopath-0.1.9-py3-none-any.whl\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.5.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.8.9)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot->detectron2) (2.4.7)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from yacs>=0.1.6->detectron2) (5.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (2.8.1)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (57.0.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (0.29.23)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 35.1MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.36.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.4.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.12.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.34.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.31.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->detectron2) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2021.5.30)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (4.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2) (3.7.4.3)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.3.post20210317-cp37-none-any.whl size=58543 sha256=ba5d2d687f42918a30676a23cec1bb2f72af5ed3ee8c1ab3db3071ae2ee1c4ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/ee/3a/5c531df777c03d8c67f22c65f97d6f75321087482d05a9b218\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=568cdaf5b2eb49b3a6d40842c30553fe40de9e2f9b335a8b10334678dfa3936d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore, antlr4-python3-runtime, omegaconf, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.8 detectron2-0.4+cu101 fvcore-0.1.3.post20210317 iopath-0.1.9 omegaconf-2.1.0 portalocker-2.3.0 yacs-0.1.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "******************************\n",
            "This step is done.\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4hmGYk1dL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922487af-82ec-4213-f5ce-5e059777ed43"
      },
      "source": [
        "#import pytorch check pytorch installation: \n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "assert torch.__version__.startswith(\"1.8\")   # please manually install torch 1.8 if Colab changes its default version cause Detectron2 currently needs torch 1.8\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.0+cu101 False\n",
            "\n",
            "\n",
            "******************************\n",
            "This step is done.\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60b6465-9321-4e7d-ef36-7dbae8da6249"
      },
      "source": [
        "# import some common pre-installed libraries\n",
        "import numpy as np\n",
        "import numpy.linalg as la\n",
        "import matplotlib.pylab as pt\n",
        "import os, json, cv2, random\n",
        "from urllib.request import urlopen # lib that handles url stuff\n",
        "from google.colab.patches import cv2_imshow\n",
        "import json, pprint\n",
        "from IPython.display import YouTubeVideo, HTML, display\n",
        "from base64 import b64encode\n",
        "from google.colab import files\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import glob\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "******************************\n",
            "This step is done.\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_zRt1VZWuRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6a8614-1513-4191-f271-23e6be01cfd1"
      },
      "source": [
        "#import libraries like Detectron2 and some of its utlities that we manually installed before\n",
        "\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger #logger\n",
        "from detectron2 import model_zoo #pre-trained models\n",
        "from detectron2.engine import DefaultPredictor #for testing/inference\n",
        "from detectron2.config import get_cfg #configuration for training and testing\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode #visualize inferences\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog #for registration/metadata\n",
        "from detectron2.structures import BoxMode #format of bounding boxes\n",
        "from detectron2.engine import DefaultTrainer #training models\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset #evaluation\n",
        "from detectron2.data import build_detection_test_loader #evaluation\n",
        "from detectron2.modeling import build_model #building models\n",
        "\n",
        "# setup Detectron2 logger\n",
        "setup_logger()\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "******************************\n",
            "This step is done.\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhVrNTd5KmPS"
      },
      "source": [
        "Import custom functions and functions from A2D2 tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W95ekKheKlRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee8c2ad-19c8-46c1-8a47-670ab54867c3"
      },
      "source": [
        "#import functions from functions.py\n",
        "!wget https://raw.githubusercontent.com/FabianGermany/AutonomousDrivingDetectron2/main/functions.py -q -O functions.py\n",
        "!python functions.py\n",
        "statement_done()\n",
        "import functions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "******************************\n",
            "This step is done.\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHlifcs4VKBi"
      },
      "source": [
        "Google Colab Access to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "584OyDIbVMZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c946ac-288d-4071-c524-3e7eab6aa191"
      },
      "source": [
        "# Colab deletes files regularly or every time restarting, to prevent this use Google Drive (another option is re-download from GitHub via !wget -q)\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "print(\"Default directory is: \")\n",
        "!pwd #show current directory\n",
        "\n",
        "os.chdir(\"/content/gdrive/My Drive/Dev/ColabDetectron2Project\") #change directory\n",
        "\n",
        "print(\"Current directory is: \")\n",
        "!pwd #show current directory\n",
        "\n",
        "print(\"Files in current directory are:\\n\")\n",
        "!ls #list files in current directory\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Default directory is: \n",
            "/content\n",
            "Current directory is: \n",
            "/content/gdrive/My Drive/Dev/ColabDetectron2Project\n",
            "Files in current directory are:\n",
            "\n",
            "dataset_1_train\n",
            "dataset_2_test\n",
            "detectron2\n",
            "example-personal-input.jpg\n",
            "example-personal-output.jpg\n",
            "exemplary_scene_rural_2_muted_input_local.mp4\n",
            "exemplary_scene_rural_2_muted_output_default_local.mkv\n",
            "functions.py\n",
            "output\n",
            "__pycache__\n",
            "testing_dict.json\n",
            "training_dict.json\n",
            "\n",
            "\n",
            "******************************\n",
            "This step is done.\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN3NAoiELz4P"
      },
      "source": [
        "# 2 Choosing a pre-trained Detectron2 model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgZEf9lCgGoH"
      },
      "source": [
        "Choose a pre-trained model from the Detectron2 model zoo. For comparison purposes, we can try to run the whole script with different models such as Mask R-CNN, Faster R-CNN or RetinaNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AWHlnDVgKD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67886cdd-41af-418e-a945-37c544116cfd"
      },
      "source": [
        "#model file\n",
        "#see https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md\n",
        "\n",
        "#model_path is pointing to the .yaml file (this is the basic model staying the same after training as well)\n",
        "#--------------------------------------------------------\n",
        "#model_path = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\" #COCO Instance Segmentation with Mask R-CNN --> this is default but also includes instance segmentation which I dont need\n",
        "model_path = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\" #COCO Object Detection with Fast R-CNN --> I will choose this\n",
        "#model_path = \"COCO-Detection/retinanet_R_50_FPN_3x.yaml\" #COCO Object Detection with RetinaNet\n",
        "#model_path = \"COCO-Detection/rpn_R_50_FPN_1x.yaml\" #COCO Object Detection with RPN & Fast R-CNN; only lr sched = 1x, not = 3x available\n",
        "#model_path = \"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\" #COCO Panoptic Segmentation with FPN\n",
        "#model_path = \"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml #COCO Person Keypoint Detection wiht R-CNN\n",
        "\n",
        "#model_path_local is the absolute path to the config file in .yaml format\n",
        "#--------------------------------------------------------\n",
        "model_path_local = model_zoo.get_config_file(model_path) #e.g. /usr/local/lib/python3.7/dist-packages/detectron2/model_zoo/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\n",
        "\n",
        "\n",
        "#model_config_path and model_config_path_short is pointing to the pickle / config file (.pkl) (or sometimes .pth when it's trained) That's for the WEIGHTS file and it will change after training\n",
        "#--------------------------------------------------------\n",
        "#see #https://github.com/facebookresearch/detectron2/blob/master/detectron2/model_zoo/model_zoo.py\n",
        "model_config_path = model_zoo.get_checkpoint_url(model_path) #e.g. https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\n",
        "model_config_path_short = model_config_path[42:] #remove first part of URL; e.g. \"COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"\n",
        "\n",
        "#Besides COCO, there is also some models trained on Cityscapes & Pascal VOC or LVIS\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "******************************\n",
            "This step is done.\n",
            "******************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM1thbN-ntjI"
      },
      "source": [
        "Then, we create a Detectron2 config and a Detectron2 `DefaultPredictor`. This will be our pretrained predictor that we won't change. For comparison purposes, we will later create another cfg called `cfg2` which will also start as a pretrained model but we will train it later and compare to `cfg`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUjkwRsOn1O0"
      },
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_path_local) #use the pre-trained model\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = model_config_path\n",
        "pretrained_predictor = DefaultPredictor(cfg)\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk4gID50K03a"
      },
      "source": [
        "# 3 Running the pre-trained Detectron2 model on an exemplary image\n",
        "We do this for test and demonstration purposes before we start training and testing a huge amount of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgKyUL4pngvE"
      },
      "source": [
        "Let's download and show an interesting image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq9GY37ml1kr"
      },
      "source": [
        "# load and store personal example image\n",
        "#im = cv2.imread(\"./example-input-local.jpg\") #this wont work cause Colab always deletes locally stored files\n",
        "!wget https://raw.githubusercontent.com/FabianGermany/AutonomousDrivingDetectron2/main/input_data/example_input.jpg -q -O example-personal-input.jpg\n",
        "im = cv2.imread(\"./example-personal-input.jpg\")\n",
        "\n",
        "#make image a bit smaller for display into notebook\n",
        "imS = functions.resize_img(25, im)\n",
        "cv2_imshow(imS)\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Znrf0nfvyK8"
      },
      "source": [
        "Let's use the `DefaultPredictor` to run inference (testing) on this image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiTOZlXCvtxh"
      },
      "source": [
        "outputs = pretrained_predictor(im) #run the model on the image\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d3KxiHO_0gb"
      },
      "source": [
        "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
        "print(outputs[\"instances\"].pred_classes) #prints the classes as int\n",
        "print(outputs[\"instances\"].pred_boxes) #prints the bounding box values\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4sxWxO47WQ7"
      },
      "source": [
        "Show output as image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IRGo8d0qkgR"
      },
      "source": [
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "output_image = out.get_image()[:, :, ::-1]\n",
        "output_image_resized = functions.resize_img(20, output_image)\n",
        "cv2_imshow(output_image_resized)\n",
        "cv2.imwrite(\"./example-personal-output.jpg\", output_image)\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb2tzZiog_eB"
      },
      "source": [
        "# 4 Checking out and preparing the Audi A2D2 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIiDF1zuhLDl"
      },
      "source": [
        "Before we start training our model on our [Audi A2D2 dataset](https://www.a2d2.audi/a2d2/en.html) dataset, we want to quickly make sure, that data is accessible. The data is available on the A2D2 server. Due to the huge size of data, I downloaded the data to my local storage and and picked only a small part of it (3 out of 18 folders). Two of the folders will later be the training dataset and one will be the testing dataset. Have done this, I put this data to my Google Drive due to performance advantages. \n",
        "<!--The data is already predownloaded on an Amazon AWS S3 bucket to we can run our script on SageMaker later.-->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5P6nTYYeNu_"
      },
      "source": [
        "if (not dataset_json_available):\n",
        "  link_to_config_file = \"https://raw.githubusercontent.com/FabianGermany/AutonomousDrivingDetectron2/main/About_Audi_A2D2/cams_lidars.json\" #sensor configuration file \n",
        "\n",
        "  #download and unzip/decompress training and testing files (in my case I don't run this anymore cause the 15 GB cloud storage from Google Drive is not enough for the zip files and the raw files together\n",
        "  # so I unzipped it once and deleted the zip files again)\n",
        "  !unzip dataset_1_train.zip -d dataset_1_train #unzip Data for bounding boxes for training\n",
        "  !unzip dataset_2_test.zip -d dataset_2_test #unzip Data for bounding boxes for testing\n",
        "  \n",
        "statement_done()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFz1LWP2kjvV"
      },
      "source": [
        "if (not dataset_json_available):\n",
        "  f = urlopen(link_to_config_file) #open json file from webserver (use urlopen cause open function is python is based on relative paths on current directory)\n",
        "  config = json.load(f)\n",
        "  #print(\"This is the whole config file: \\n\")\n",
        "  #pprint.pprint(config)\n",
        "  #for specific data use something like\n",
        "  #pprint.pprint(config.keys())\n",
        "  #pprint.pprint(config['lidars'].keys())\n",
        "  #pprint.pprint(config['lidars']['front_left'])\n",
        "  #pprint.pprint(config['cameras']['front_left']['view'])\n",
        "  #etc.\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOlOPoRpaXer"
      },
      "source": [
        "Access exemplary single data (images and bounding box annotation) from dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4A8OEhgaZDW"
      },
      "source": [
        "#Choose an image\n",
        "file_name = '000000166'\n",
        "#file_name = '000002351'\n",
        "#file_name = '000004848' #missing bounding boxes...\n",
        "#file_name = '000004885' #also super bad annotation\n",
        "#file_name = '000006752'\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEo5kwNeacSH"
      },
      "source": [
        "#train folder dataset_1_train have 20181107_132730 and 20181108_091945 folders\n",
        "#test folder dataset_2_test has 20181016_125231 folder\n",
        "\n",
        "#show image\n",
        "print(\"Current image:\")\n",
        "file_img_original = 'dataset_1_train/20181107_132730/camera/cam_front_center/20181107132730_camera_frontcenter_' + file_name + '.png'\n",
        "image_original = cv2.imread(file_img_original)\n",
        "image_original_resized = functions.resize_img(40, image_original)\n",
        "cv2_imshow(image_original_resized)\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Er817M7fM52"
      },
      "source": [
        "#let's define a function that is delivering the bounding box and class information from a single entry the json file; we put this into a custom list[dict] format\n",
        "def get_bounding_boxes_and_classes_from_json(json_path, mute = True):\n",
        "\n",
        "  if(not mute): print(\"\\nBounding Box Information:\")\n",
        "  boxes = functions.read_bounding_boxes(json_path, mute = mute)\n",
        "  #points = functions.get_points(boxes[0])\n",
        "\n",
        "  if(not mute): print(\"\\n\\nJSON File Name:\")\n",
        "  if(not mute): print (json_path)\n",
        "\n",
        "  #if(not mute): print(\"\\n\\nBounding Box Information:\")\n",
        "  #if(not mute): pprint.pprint(boxes)\n",
        "\n",
        "  n_detected_objects = len(boxes)\n",
        "\n",
        "  if(not mute): print(\"\\n\\n2D Bounding Box Coordinates:\")\n",
        "\n",
        "  current_Coord2D_dict =  {'right': '', 'left': '', 'bottom': '', 'top': '', 'class': ''}\n",
        "  #Coord2D = [{}] * n_detected_objects #init a list of empty dictionaries\n",
        "  Coord2D = []\n",
        "\n",
        "  #write values into the dict entries (each bounding box has one list entry)\n",
        "  for i in range(n_detected_objects):\n",
        "    current_Coord2D_dict['right'] = boxes[i]['right']\n",
        "    current_Coord2D_dict['left'] = boxes[i]['left']\n",
        "    current_Coord2D_dict['bottom'] = boxes[i]['bottom']\n",
        "    current_Coord2D_dict['top'] = boxes[i]['top']\n",
        "    current_Coord2D_dict['class'] = boxes[i]['class']\n",
        "    Coord2D.append(current_Coord2D_dict.copy()) #use copy(): You need to append a copy, otherwise you are just adding references to the same dictionary over and over again:\n",
        "\n",
        "  return Coord2D, n_detected_objects\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odTpg4QeIhim"
      },
      "source": [
        "#let's run this function on a test json\n",
        "file_name_bboxes = 'dataset_1_train/20181107_132730/label3D/cam_front_center/20181107132730_label3D_frontcenter_' + file_name + '.json'\n",
        "result_json, n_objects = get_bounding_boxes_and_classes_from_json(file_name_bboxes, mute = False)\n",
        "pprint.pprint(result_json)\n",
        "\n",
        "#print(\"\\n\\n3D Coordinates:\")\n",
        "#print(points)\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtr_2WD5fR0b"
      },
      "source": [
        "#Double check: Bounding Box on image\n",
        "print(\"Image with 2D bounding boxes:\")\n",
        "file_img_original = 'dataset_1_train/20181107_132730/camera/cam_front_center/20181107132730_camera_frontcenter_' + file_name + '.png'\n",
        "image_original = cv2.imread(file_img_original)\n",
        "\n",
        "\n",
        "#amount of boxes\n",
        "n_detected_objects = len(result_json)\n",
        "\n",
        "#draw bounding boxes\n",
        "for i in range(n_detected_objects):\n",
        "  cv2.rectangle(image_original, (int(result_json[i]['top']), int(result_json[i]['left'])), (int(result_json[i]['bottom']), int(result_json[i]['right'])),(36,255,12), 4) #according to documentation: (left, top) and (right, bottom), but doesn't work; seems like A2D2 dataset has some mix-up\n",
        "  cv2.putText(image_original, result_json[i]['class'], (int(result_json[i]['top']), int(result_json[i]['left'])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
        "  #cv2.circle(image_original, (100,900), radius=4, color=(36,255,12), thickness=5)\n",
        "\n",
        "image_original_resized = functions.resize_img(40, image_original)\n",
        "cv2_imshow(image_original_resized)\n",
        "\n",
        "#the bounding box fits roughly, but it's actually not very good (reason: maybe the conversion to int?)\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXUUWIL0bWcd"
      },
      "source": [
        "The annotations of A2D2 are actually sometimes pretty bad and somtimes missing, like in the files '000004848' or '000004885."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AWLzLZ2XNBZ"
      },
      "source": [
        "Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBjrsw5XXKXO"
      },
      "source": [
        "if (dataset_json_available): #already done before, so just load it\n",
        "  dict_of_training_images_and_json = json.load(open(\"training_dict.json\"))\n",
        "  dict_of_testing_images_and_json = json.load(open(\"testing_dict.json\"))\n",
        "\n",
        "else: #not created before, so let's recreate\n",
        "\n",
        "  #train folder dataset_1_train have 20181107_132730 and 20181108_091945 folders\n",
        "  #test folder dataset_2_test has 20181016_125231 folder\n",
        "\n",
        "  #the training images and json are here:\n",
        "  path_training_images_1 = 'dataset_1_train/20181107_132730/camera/cam_front_center/' #/20181107132730_camera_frontcenter_...\n",
        "  path_training_json_1 = 'dataset_1_train/20181107_132730/label3D/cam_front_center/' #/20181107132730_label3D_frontcenter_...\n",
        "  path_training_images_2 = 'dataset_1_train/20181108_091945/camera/cam_front_center/' #/20181108091945_camera_frontcenter_...\n",
        "  path_training_json_2 = 'dataset_1_train/20181108_091945/label3D/cam_front_center/'#/20181108091945_label3D_frontcenter_...\n",
        "\n",
        "  #the testing images and json are here:\n",
        "  path_testing_images = 'dataset_2_test/20181016_125231/camera/cam_front_center/' #/20181016125231_camera_frontcenter_...\n",
        "  path_testing_json = 'dataset_2_test/20181016_125231/label3D/cam_front_center/' #/20181016125231_label3D_frontcenter_...\n",
        "\n",
        "  #list up all elements in those folders\n",
        "  list_of_training_images_1 = [f for f in listdir(path_training_images_1) if isfile(join(path_training_images_1, f))]\n",
        "  list_of_training_json_1 = [f for f in listdir(path_training_json_1) if isfile(join(path_training_json_1, f))]\n",
        "  list_of_training_images_2 = [f for f in listdir(path_training_images_2) if isfile(join(path_training_images_2, f))]\n",
        "  list_of_training_json_2 = [f for f in listdir(path_training_json_2) if isfile(join(path_training_json_2, f))]\n",
        "  list_of_testing_images = [f for f in listdir(path_testing_images) if isfile(join(path_testing_images, f))]\n",
        "  list_of_testing_json = [f for f in listdir(path_testing_json) if isfile(join(path_testing_json, f))]\n",
        "\n",
        "  #remove json information files from image folders\n",
        "  list_of_training_images_1 = [x for x in list_of_training_images_1 if x.endswith('png')]\n",
        "  list_of_training_images_2 = [x for x in list_of_training_images_2 if x.endswith('png')]\n",
        "  list_of_testing_images = [x for x in list_of_testing_images if x.endswith('png')]\n",
        "\n",
        "  #add the path to the file into the list to get the full path into the dict later\n",
        "  list_of_training_images_1_full_path = []\n",
        "  list_of_training_json_1_full_path = []\n",
        "  list_of_training_images_2_full_path = []\n",
        "  list_of_training_json_2_full_path = []\n",
        "  list_of_testing_images_full_path = []\n",
        "  list_of_testing_json_full_path = []\n",
        "\n",
        "  for entry in list_of_training_images_1:\n",
        "    list_of_training_images_1_full_path.append(path_training_images_1 + str(entry))\n",
        "  for entry in list_of_training_json_1:\n",
        "    list_of_training_json_1_full_path.append(path_training_json_1 + str(entry))\n",
        "  for entry in list_of_training_images_2:\n",
        "    list_of_training_images_2_full_path.append(path_training_images_2 + str(entry))\n",
        "  for entry in list_of_training_json_2:\n",
        "    list_of_training_json_2_full_path.append(path_training_json_2 + str(entry))\n",
        "  for entry in list_of_testing_images:\n",
        "    list_of_testing_images_full_path.append(path_testing_images + str(entry))\n",
        "  for entry in list_of_testing_json:\n",
        "    list_of_testing_json_full_path.append(path_testing_json + str(entry))\n",
        "\n",
        "  #merge the two training datasets\n",
        "  list_of_training_images_full_path = list_of_training_images_1_full_path + list_of_training_images_2_full_path\n",
        "  list_of_training_json_full_path = list_of_training_json_1_full_path + list_of_training_json_2_full_path\n",
        "\n",
        "  #sorts lists alpabetically (so by number of the file)\n",
        "  list_of_training_images_full_path = sorted(list_of_training_images_full_path, key=str.lower)\n",
        "  list_of_training_json_full_path = sorted(list_of_training_json_full_path, key=str.lower)\n",
        "  list_of_testing_images_full_path = sorted(list_of_testing_images_full_path, key=str.lower)\n",
        "  list_of_testing_json_full_path = sorted(list_of_testing_json_full_path, key=str.lower)\n",
        "\n",
        "  #make pairs of png file and json file\n",
        "  dict_of_training_images_and_json, dict_of_testing_images_and_json = [], []\n",
        "  current_training_img_and_json, current_testing_img_and_json = {'image': '', 'json': ''}, {'image': '', 'json': ''}\n",
        "\n",
        "  for current_training_image, current_training_json in zip(list_of_training_images_full_path, list_of_training_json_full_path):\n",
        "    current_training_img_and_json['image'] = current_training_image\n",
        "    current_training_img_and_json['json'] = current_training_json\n",
        "    dict_of_training_images_and_json.append(current_training_img_and_json.copy())\n",
        "\n",
        "  for current_testing_image, current_testing_json in zip(list_of_testing_images_full_path, list_of_testing_json_full_path):\n",
        "    current_testing_img_and_json['image'] = current_testing_image\n",
        "    current_testing_img_and_json['json'] = current_testing_json\n",
        "    dict_of_testing_images_and_json.append(current_testing_img_and_json.copy())\n",
        "\n",
        "  #store dict_of_testing_images_and_json and dict_of_training_images_and_json\n",
        "  json.dump( dict_of_training_images_and_json, open( \"training_dict.json\", 'w' ) )\n",
        "  json.dump( dict_of_testing_images_and_json, open( \"testing_dict.json\", 'w' ) )\n",
        "\n",
        "\n",
        "#print the dict\n",
        "print('Paths to training data files: ' + str(dict_of_training_images_and_json))\n",
        "print('Paths to testing data files: ' + str(dict_of_testing_images_and_json))\n",
        "#pprint.pprint(dict_of_training_images_and_json)\n",
        "\n",
        "#amount of training and testing images\n",
        "n_training_elements = len(dict_of_training_images_and_json)\n",
        "n_testing_elements = len(dict_of_testing_images_and_json)\n",
        "print(\"Amount of training images: \" + str(n_training_elements))\n",
        "print(\"Amount of testing images: \" + str(n_testing_elements))\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luv5TATeDOSV"
      },
      "source": [
        "#We need a function converting the class into a number (Detectron2 needs that format)\n",
        "#According to detectron documentation: category_id (int, required): an integer in the range [0, num_categories-1] representing the category label. The value num_categories is reserved to represent the “background” category, if applicable.\n",
        "#the docs of A2D2 only list up all segmentation classes, but not the bounding box classes, they just tell on the website there is 14; \n",
        "#so I searched manually for it and found in my 3 sub-datasets 12 classes (either I missed something or the sub-datasets doesn't include the remaining 2 categories)\n",
        "#but well it doesn't matter, so remaining text, will receive another number meaning \"misc\"\n",
        "\n",
        "def conv_num(class_name_as_string):\n",
        "  if(class_name_as_string == 'Animal'):\n",
        "    return 0\n",
        "  elif(class_name_as_string == 'Bicycle'):\n",
        "    return 1\n",
        "  elif(class_name_as_string == 'Bus'):\n",
        "    return 2\n",
        "  elif(class_name_as_string == 'Car'):\n",
        "    return 3\n",
        "  elif(class_name_as_string == 'Cyclist'):\n",
        "    return 4\n",
        "  elif(class_name_as_string == 'EmergencyVehicle'):\n",
        "    return 5\n",
        "  elif(class_name_as_string == 'MotorBiker'):\n",
        "    return 6\n",
        "  elif(class_name_as_string == 'Motorcycle'):\n",
        "    return 7\n",
        "  elif(class_name_as_string == 'Pedestrian'):\n",
        "    return 8\n",
        "  elif(class_name_as_string == 'Truck'):\n",
        "    return 9\n",
        "  elif(class_name_as_string == 'UtilityVehicle'):\n",
        "    return 10\n",
        "  elif(class_name_as_string == 'VanSUV'):\n",
        "    return 11\n",
        "  else: #maybe I missed Nr. 13/14, but it doesn't matter cause this class doesn't seem to be relevant\n",
        "    return 12\n",
        "\n",
        "\n",
        "#this means we have 0...12 = 13 classes (including the misc class)\n",
        "#this means 14 will be the background category for Detectron2\n",
        "\n",
        "bb_classes = ['Animal', 'Bicycle', 'Bus', 'Car', 'Cyclist', \n",
        "              'EmergencyVehicle', 'MotorBiker', 'Motorcycle', \n",
        "              'Pedestrian', 'Truck', 'UtilityVehicle', 'VanSUV', 'Misc']\n",
        "\n",
        "n_classes = len(bb_classes)\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVJoOm6LVJwW"
      },
      "source": [
        "Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into detectron2's standard format. User should write such a function when using a dataset in custom format. See the [Detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html) for more details. We also need to register our dataset to Detectron2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2ogNh3bjEFm"
      },
      "source": [
        "#Detectron needs a function returning the dataset in list[dict] format which we already created; more information: https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html\n",
        "#input is a list[dict] as well, but in another format; so this function converts our list[dict] to a format simular to the COCO format\n",
        "def Audi_Dataset_Function(data_in_custom_format, demonstration_mode = False):\n",
        "\n",
        "  #ABOUT\n",
        "  #----------------------------\n",
        "  #the format needs to be list[dict] according to COCO's format; each dict is representing one image\n",
        "  #we need the fields: file_name, height, width, image_id and annotations\n",
        "  \n",
        "  '''\n",
        "  file_name #full path to image (str)\n",
        "  height, width #shape of image (int)\n",
        "  image_id #unique ID (str or int)\n",
        "  annotations #available annotations(list[dict])\n",
        "    bbox #4 numbers for bounding box (list[float])\n",
        "    bbox_mode #format of the bbox(int), currently supports: BoxMode.XYXY_ABS, BoxMode.XYWH_ABS\n",
        "    category_id #an integer in the range [0, num_categories-1] representing the category label. The value num_categories is reserved to represent the “background” category, if applicable.(int)\n",
        "    segmentation, keypoints, iscrowd #this is not needed for our purposes\n",
        "  '''\n",
        "\n",
        "  #for the demonstration mode only use a part of data\n",
        "  if (demonstration_mode == False): #normal mode\n",
        "    n_iterations = len(data_in_custom_format) #use all elements\n",
        "  else: #demonstration mode\n",
        "    n_iterations = 10 #use only a part of the data\n",
        "    print(\"DEMONSTRATION MODE\")\n",
        "    #alternative for demonstration mode: for d in random.sample(range(n_iterations), 3):\n",
        "\n",
        "\n",
        "  #(1) first we create an empty list of dict\n",
        "  dataset = []\n",
        "\n",
        "  #(2) now for each iteration (scene/image) we create the required dict and grab the current image and json file and use its data\n",
        "  \n",
        "  for i in range(n_iterations):\n",
        "    print(\"Iteration \" + str(i+1) + \" / \" + str(n_iterations))\n",
        "\n",
        "    #(2.1) We create an empty dict\n",
        "    current_dict = {}\n",
        "\n",
        "    #(2.2) now for each image/json we extract the annotation data and convert to desired format\n",
        "    path_to_image_file = data_in_custom_format[i]['image'] #image file name\n",
        "    path_to_json_file = data_in_custom_format[i]['json'] #json file name\n",
        "    BBox_Output, n_available_bounding_boxes = get_bounding_boxes_and_classes_from_json(path_to_json_file) #json data\n",
        "    height, width = cv2.imread(path_to_image_file).shape[:2] #height and width\n",
        "\n",
        "    current_dict[\"file_name\"] = path_to_image_file\n",
        "    current_dict[\"height\"] = height\n",
        "    current_dict[\"width\"] = width\n",
        "    current_dict[\"image_id\"] = i #or we could also use the filename etc.\n",
        "\n",
        "    \n",
        "    #(2.3) first we create an empty list of dict for the annotations\n",
        "    annotations = []\n",
        "\n",
        "    #(2.4) now for each available bounding box we extract the data\n",
        "\n",
        "    for j in range(n_available_bounding_boxes):\n",
        "\n",
        "      #(2.4.1) first we extract the data\n",
        "      BBox_Top = BBox_Output[j]['top']\n",
        "      BBox_Bottom = BBox_Output[j]['bottom']\n",
        "      BBox_Left = BBox_Output[j]['left']\n",
        "      BBox_Right = BBox_Output[j]['right']\n",
        "      BBox_class = BBox_Output[j]['class']\n",
        "      BBox_class_numeric = conv_num(BBox_class)\n",
        "\n",
        "      current_annotation = {\n",
        "            \"bbox\": [BBox_Top, BBox_Left, BBox_Bottom, BBox_Right], #TODO I changed the sequence (see bbox_mode)\n",
        "            \"bbox_mode\": BoxMode.XYXY_ABS, #we need XYXY_ABS which means: (x0, y0, x1, y1) in absolute floating points coordinates; should be left, top, right, bottom; but in this case I guess top, left, bottom, right\n",
        "            \"category_id\": BBox_class_numeric, #class as int type\n",
        "      }\n",
        "\n",
        "      #(2.4.2) then we append this annotation dict to the annotation list\n",
        "      annotations.append(current_annotation.copy())\n",
        "\n",
        "    #(2.5) then we add the annotation list to the current dict\n",
        "    current_dict[\"annotations\"] = annotations\n",
        "\n",
        "    #(2.6) then we append this dict to the list\n",
        "    dataset.append(current_dict.copy())\n",
        "\n",
        "  #(3) we finish and return the whole dataset\n",
        "  return dataset\n",
        "\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "608EfaiTb00q"
      },
      "source": [
        "#test this function before run with all data\n",
        "output_dict_demonstration = Audi_Dataset_Function(dict_of_training_images_and_json, demonstration_mode = True)\n",
        "pprint.pprint(output_dict_demonstration)\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M24ht0FpOeGd"
      },
      "source": [
        "# register our training dataset to Detectron2\n",
        "d = 'training'\n",
        "DatasetCatalog.register(\"Custom_Audi_A2D2_Dataset_Training\", lambda d = d : Audi_Dataset_Function(dict_of_training_images_and_json, demonstration_mode = False)) #register the whole training dataset; attention: this only works with lambda functions!\n",
        "MetadataCatalog.get(\"Custom_Audi_A2D2_Dataset_Training\").set(thing_classes = bb_classes) #declare class names\n",
        "\n",
        "Audi_Metadata = MetadataCatalog.get(\"Custom_Audi_A2D2_Dataset_Training\") #register metadata of training dataset (might also be used for testing/evaluation)\n",
        "#access the data via: List[Dict] = DatasetCatalog.get(\"Custom_Audi_A2D2_Dataset_Training\")\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ljbWTX0Wi8E"
      },
      "source": [
        "To verify the data loading is correct, let's visualize the images and its annotations of some selected samples in the training set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkNbUzUOLYf0"
      },
      "source": [
        "output_dict_demonstration = Audi_Dataset_Function(dict_of_training_images_and_json, demonstration_mode = True)\n",
        "for d in output_dict_demonstration:\n",
        "    print(\"Image \" + str(d[\"file_name\"] + \":\"))\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=Audi_Metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    output_img = out.get_image()[:, :, ::-1]\n",
        "    output_img_resized = functions.resize_img(50, output_img)\n",
        "    cv2_imshow(output_img_resized)\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bjrfb2LDeo"
      },
      "source": [
        "# 5 Training Detectron2 model on Audi A2D2 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjbUIhSxUdm_"
      },
      "source": [
        "The previous steps were successful, so we can now train a Detectron2 model on our custom Audi A2D2 dateset.\n",
        "This dataset has 14 classes for bounding boxes.\n",
        "We'll train a bounding box model from an existing model pre-trained on COCO dataset, available in detectron2's model zoo.\n",
        "\n",
        "Note that COCO dataset does not have categories like \"EmergencyVehicle\". But we'll be able to recognize these new classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKRYj5aWBz2d"
      },
      "source": [
        "We have a COCO-pretrained R50-FPN Faster R-CNN model that we will fine-tune now on the A2D2 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7unkuuiqLdqd"
      },
      "source": [
        "if (not load_existing_trained_model): #learn again\n",
        "  cfg2 = get_cfg()\n",
        "  cfg2.merge_from_file(model_path_local) #use the COCO-pretrained model from model zoo\n",
        "  cfg2.DATASETS.TRAIN = (\"Custom_Audi_A2D2_Dataset_Training\",) #use the A2D2 dataset to train\n",
        "  cfg2.DATASETS.TEST = () #no test for now\n",
        "  cfg2.DATALOADER.NUM_WORKERS = 2\n",
        "  cfg2.MODEL.WEIGHTS = model_config_path  #initialize with COCO-pretrained model from model zoo \n",
        "  cfg2.SOLVER.IMS_PER_BATCH = 2\n",
        "  cfg2.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "  cfg2.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough, but training longer might be better\n",
        "  cfg2.SOLVER.STEPS = []        # do not decay learning rate\n",
        "  cfg2.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this dataset (default: 512)\n",
        "  cfg2.MODEL.ROI_HEADS.NUM_CLASSES = n_classes  # amount of classes (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "\n",
        "  os.makedirs(cfg2.OUTPUT_DIR, exist_ok=True) #save trained model here\n",
        "  trainer = DefaultTrainer(cfg2) \n",
        "  trainer.resume_or_load(resume=False) #todo this can also be used for loading model...; resume=true means loading from MODEL.WEIGHTS\n",
        "  trainer.train()\n",
        "\n",
        "else: #load_existing_trained_model #use existing model to skip some steps and save time\n",
        "\n",
        "  cfg2 = get_cfg()\n",
        "  cfg2.merge_from_file(model_path_local)\n",
        "  #cfg2.DATASETS.TRAIN = (\"Custom_Audi_A2D2_Dataset_Training\",) #use the A2D2 dataset to train\n",
        "  #cfg2.DATASETS.TRAIN = ()\n",
        "  cfg2.DATASETS.TEST = () #no test for now\n",
        "  cfg2.DATALOADER.NUM_WORKERS = 2\n",
        "  #cfg2.MODEL.WEIGHTS = model_config_path  #initialize with COCO-pretrained model from model zoo \n",
        "  cfg2.MODEL.WEIGHTS = os.path.join(cfg2.OUTPUT_DIR, \"model_final.pth\")  # path to the model we trained before\n",
        "  cfg2.SOLVER.IMS_PER_BATCH = 2\n",
        "  cfg2.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "  cfg2.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough, but training longer might be better\n",
        "  cfg2.SOLVER.STEPS = []        # do not decay learning rate\n",
        "  cfg2.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this dataset (default: 512)\n",
        "  cfg2.MODEL.ROI_HEADS.NUM_CLASSES = n_classes  # amount of classes (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "\n",
        "\n",
        "  #trainer = DefaultTrainer(cfg2) #this is also needed for evaluation TODO...todo läuft nur wenn cfg2.DATASETS.TRAIN dabei ist... #todo this step is super slow, can I skip it?\n",
        "  #trainer.resume_or_load(resume=True) #todo set to true if only evaluate\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBXeH8UXFcqU"
      },
      "source": [
        "# Look at training curves in tensorboard:\n",
        "if (not load_existing_trained_model): \n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir output\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0aeEBI6dZOe"
      },
      "source": [
        "# 6 Testing trained model (Inference & evaluation using the trained model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD5-5Lpxdgih"
      },
      "source": [
        "After then model has been trained, we need to evaluate it by running inference with the trained model on the A2D2 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32M_SBnaBkqB"
      },
      "source": [
        "First, we create a predictor using the model we just trained:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya5nEuMELeq8"
      },
      "source": [
        "# Inference should use the config with parameters that are used in training\n",
        "# cfg2 already exists and already contains everything we've set previously. We changed it a little bit for inference:\n",
        "cfg2.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3   # set a custom testing threshold #default was 0.7\n",
        "cfg2.MODEL.WEIGHTS = os.path.join(cfg2.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "#cfg2.DATASETS.TEST = (\"fruits_nuts\", ) #todo das kommt von oben...\n",
        "finetuned_predictor = DefaultPredictor(cfg2)\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWq1XHfDWiXO"
      },
      "source": [
        "Then, we randomly select several samples to visualize the prediction results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5LhISJqWXgM"
      },
      "source": [
        "output_dict_testing_selection = Audi_Dataset_Function(dict_of_testing_images_and_json, demonstration_mode = True)\n",
        "for d in output_dict_testing_selection:\n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = finetuned_predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    v = Visualizer(im[:, :, ::-1], metadata=Audi_Metadata, scale=0.5)\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    output_img = out.get_image()[:, :, ::-1]\n",
        "    output_img_resized = functions.resize_img(70, output_img)\n",
        "    cv2_imshow(output_img_resized)\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBvSmWRhXctk"
      },
      "source": [
        "Let's register our testing dataset now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN6raE3YXEcc"
      },
      "source": [
        "# register the testing dataset here\n",
        "e = 'testing'\n",
        "DatasetCatalog.register(\"Custom_Audi_A2D2_Dataset_Testing\", lambda e = e : Audi_Dataset_Function(dict_of_testing_images_and_json, demonstration_mode = False)) #register whole training dataset\n",
        "MetadataCatalog.get(\"Custom_Audi_A2D2_Dataset_Testing\").set(thing_classes = bb_classes) #declare class names\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kblA1IyFvWbT"
      },
      "source": [
        "We will evaluate its performance using AP metric implemented in COCO API.\n",
        "The value goes from 0...100 (instead of 0...1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JPx0SEZfLSo"
      },
      "source": [
        "evaluator = COCOEvaluator(\"Custom_Audi_A2D2_Dataset_Testing\", (\"bbox\", ), False, output_dir=\"./output/\") #tasks needs to be tuple format:(\"bbox\", \"segm\") etc. ; if (\"bbox\",) you need to have comma, otherwise it will be read as string\n",
        "val_loader = build_detection_test_loader(cfg2, \"Custom_Audi_A2D2_Dataset_Testing\")\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9tECBQCvMv3"
      },
      "source": [
        "if (load_existing_trained_model):\n",
        "  model = build_model(cfg2)  # returns a torch.nn.Module #todo is that correct? \n",
        "  #https://detectron2.readthedocs.io/en/latest/_modules/detectron2/evaluation/evaluator.html#inference_on_dataset\n",
        "  #If model is an nn.Module, it will be temporarily set to `eval` mode.\n",
        "  #If you wish to evaluate a model in `training` mode instead, you can wrap the given model and override its behavior of `.eval()` and `.train()`.\n",
        "  #inference_on_dataset needs a argument model which is usually trainer.model;\n",
        "  # but if we don't train before we don't have this\n",
        "  print(inference_on_dataset(model, val_loader, evaluator))\n",
        "\n",
        "else:\n",
        "  print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
        "  # another equivalent way to evaluate the model is to use `trainer.test`\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXJncJ7aal8A"
      },
      "source": [
        "The metrics are given by the text above. The frequency information in FPS is hidden next to : \n",
        "`Total inference pure compute time`.\n",
        "For Faster R-CNN we get `Total inference pure compute time: 0:01:51 (0.126056 s / img per device, on 1 devices)`\n",
        "\n",
        "COCOEvaluator is delivering AP values from 0 to 100. According to [COCO](https://cocodataset.org/#detection-eval), there is no difference between AP and mAP, so let's take the final AP value for our evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXadAb9Fv-L"
      },
      "source": [
        "# 7 Bonus: Running the model on a video\n",
        "Let's run our original and our trained model on an exemplary self-made video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDif9_0nu5uX"
      },
      "source": [
        "# We will process a video stored in my GitHub (it's also possible to use video from YouTube, for that see the Detectron2 Colab tutorial)\n",
        "!wget https://raw.githubusercontent.com/FabianGermany/AutonomousDrivingDetectron2/main/input_data/exemplary_scene_rural_2_muted-input.mp4 -q -O exemplary_scene_rural_2_muted_input_local.mp4\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWufmIqyVFPG"
      },
      "source": [
        "#show the original video\n",
        "mp4 = open('./exemplary_scene_rural_2_muted_input_local.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "display(HTML('Original video:'))\n",
        "display(HTML(\"\"\"\n",
        "<video width=700 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url))\n",
        "\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t64l42zLzAEK"
      },
      "source": [
        "First we use the original model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O4T1HxyAnnJ"
      },
      "source": [
        "# Run frame-by-frame inference demo on this video with the \"demo.py\" tool provided by Detectron2:\n",
        "!git clone https://github.com/facebookresearch/detectron2\n",
        "\n",
        "%run detectron2/demo/demo.py --config-file detectron2/configs/{model_path} --video-input exemplary_scene_rural_2_muted_input_local.mp4 --confidence-threshold 0.6 --output exemplary_scene_rural_2_muted_output_default_local.mkv \\\n",
        "  --opts MODEL.WEIGHTS detectron2://\\{model_config_path_short}\n",
        "#if you want to use semantic segmentation, panoptic segmentation etc. just change model_path and model_config_path_short variable \n",
        "statement_done()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLYWqtT5ApFR"
      },
      "source": [
        "# Download the results\n",
        "files.download('exemplary_scene_rural_2_muted_output_default_local.mkv')\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHFvxbUlzEJZ"
      },
      "source": [
        "Now we use our trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT3wZSamzGGP"
      },
      "source": [
        "#!git clone https://github.com/facebookresearch/detectron2 #todo delete this line?\n",
        "%run detectron2/demo/demo.py --config-file detectron2/configs/{model_path} --video-input exemplary_scene_rural_2_muted_input_local.mp4 --confidence-threshold 0.4 --output exemplary_scene_rural_2_muted_output_trained_local.mkv \\\n",
        "  --opts MODEL.WEIGHTS os.path.join(cfg2.OUTPUT_DIR, \"model_final.pth\")\n",
        "statement_done()\n",
        "\n",
        "#--opts MODEL.WEIGHTS detectron2://\\{model_config_path_short} --> .pkl --> diese sind eig. hier: \"model_final.pth\" denn oben: cfg2.MODEL.WEIGHTS = os.path.join(cfg2.OUTPUT_DIR, \"model_final.pth\")\n",
        "#todo adapt threshold\n",
        "#todo try to outsource: path_weights_trained = os.path.join(cfg2.OUTPUT_DIR, \"model_final.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZAiRFw_zGVV"
      },
      "source": [
        "# Download the results\n",
        "files.download('exemplary_scene_rural_2_muted_output_trained_local.mkv')\n",
        "statement_done()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}